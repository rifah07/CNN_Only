{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e350cdc-e949-4b93-b22c-73ec07a90781",
   "metadata": {
    "id": "8e350cdc-e949-4b93-b22c-73ec07a90781"
   },
   "source": [
    "# Dense layer is a fundamental building block of neural networks in Keras, a popular deep learning library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061eb735-c772-4321-a8d7-bf7fa51d49da",
   "metadata": {
    "id": "061eb735-c772-4321-a8d7-bf7fa51d49da"
   },
   "source": [
    "# **Dense Layer:** A Dense layer is also known as a fully connected layer. It is a layer where each neuron is connected to every neuron in the previous layer. This means that each input node is connected to each output node by a weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa496006-ced7-4f23-a2c4-77066fec108f",
   "metadata": {
    "id": "fa496006-ced7-4f23-a2c4-77066fec108f"
   },
   "source": [
    "# Here's a quick breakdown of what it does:\n",
    "\n",
    "\n",
    "**Weights and Biases:** Each connection has an associated weight, which gets adjusted during training to minimize the error of the network. Bias terms are also included to provide additional flexibility in learning.\n",
    "\n",
    "Activation Functions: **bold text** The Dense layer also usually applies an activation function to the weighted sum of the inputs to introduce non-linearity, which helps the network learn more complex patterns. Common activation functions include **ReLU, sigmoid, **and** softmax.**\n",
    "\n",
    "**Output Shape:** The number of neurons in the Dense layer determines the output shape of the layer. The shape of the output is always (batch_size, number_of_neurons_in_layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3655026d-6635-4be5-93a9-bee9be5ef214",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3655026d-6635-4be5-93a9-bee9be5ef214",
    "outputId": "9348bd66-9ef8-4955-8637-30d9894fbc6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3d82a-3882-423f-8c11-11818e664739",
   "metadata": {
    "id": "16c3d82a-3882-423f-8c11-11818e664739"
   },
   "source": [
    "# In this example:\n",
    "\n",
    "**First Dense Layer:**\n",
    "\n",
    "units=64: This means the layer has 64 neurons.\n",
    "\n",
    "input_dim=100: This means the input to this layer has 100 features (or dimensions).\n",
    "\n",
    "Output Shape: Since this layer has 64 neurons, it will output a shape of (batch_size, 64). The batch_size is determined by how many samples you feed into the model at one time, but the key part here is that the number of neurons (64) is the determining factor for the shape.\n",
    "The batch_size is a concept in machine learning and deep learning that refers to the number of training examples utilized in one iteration.\n",
    "\n",
    "**Second Dense Layer:**\n",
    "\n",
    "units=10: This means the layer has 10 neurons.\n",
    "\n",
    "Output Shape: Since this layer has 10 neurons, it will output a shape of (batch_size, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qadBnj5OTZH8",
   "metadata": {
    "id": "qadBnj5OTZH8"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training data\n",
    "X_train = [...] # Your training data features\n",
    "y_train = [...] # Your training data labels\n",
    "\n",
    "# Fit the model with a specified batch_size\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aEJu83gTrHa",
   "metadata": {
    "id": "3aEJu83gTrHa"
   },
   "source": [
    "In this example:\n",
    "\n",
    "batch_size=32: This means the model will process 32 samples at a time during training. If your dataset has 1,000 samples, the model will process the data in *chunks of 32 samples per iteration*.\n",
    "\n",
    "epochs=10: The model will iterate over the entire dataset 10 times.\n",
    "\n",
    "You can adjust the batch_size according to your needs and resources. Smaller batch sizes can lead to more frequent updates, while larger batch sizes can take advantage of parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77LlNSDnTa9J",
   "metadata": {
    "id": "77LlNSDnTa9J"
   },
   "source": [
    "# The batch_size is a concept in machine learning and deep learning that refers to the number of training examples utilized in one iteration.\n",
    "\n",
    "Here's a more detailed explanation:\n",
    "\n",
    "**Training Process:** When training a neural network, data is processed in batches rather than one sample at a time or all samples at once. Processing data in batches can speed up the training process and make it more efficient.\n",
    "\n",
    "**Batch:** A batch is a subset of the training data. Instead of feeding the entire dataset into the model, we divide it into smaller chunks or batches.\n",
    "\n",
    "**Batch Size:** The batch_size is the number of samples in each batch. For example, if you have 1,000 samples and a batch_size of 32, then the model will process 32 samples at a time, iterating over the dataset in chunks of 32 samples until all samples have been used."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
